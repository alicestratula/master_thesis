{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6543b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Merge all your CSV files into one DataFrame ---\n",
    "\n",
    "# IMPORTANT: Set this to the main folder containing your result CSVs.\n",
    "# This script assumes the CSVs might be in subdirectories inside this path.\n",
    "RESULTS_DIRECTORY = r'PATH/TO/YOUR/RESULTS' # <-- CHANGE THIS\n",
    "\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(RESULTS_DIRECTORY):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.csv'):\n",
    "            all_files.append(os.path.join(root, filename))\n",
    "\n",
    "# Read and combine all found CSV files\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "master_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"Successfully loaded and merged {len(all_files)} CSV files.\")\n",
    "print(\"Master DataFrame head:\")\n",
    "print(master_df.head())\n",
    "\n",
    "\n",
    "# --- 2. Calculate the Relative Difference ---\n",
    "\n",
    "# IMPORTANT: Choose the metric you want to analyze.\n",
    "# For metrics where lower is better (like RMSE, LogLoss), use 'minimize'.\n",
    "# For metrics where higher is better (like Accuracy), use 'maximize'.\n",
    "METRIC_TO_ANALYZE = 'LogLoss'  # <-- CHANGE THIS (e.g., to 'Accuracy')\n",
    "OPTIMIZATION_GOAL = 'minimize' # <-- CHANGE THIS (to 'maximize' for Accuracy)\n",
    "\n",
    "# Filter the DataFrame to only contain the metric of interest\n",
    "df_metric = master_df[master_df['metric'] == METRIC_TO_ANALYZE].copy()\n",
    "\n",
    "# Define a function to calculate relative difference within each group\n",
    "def calculate_relative_difference(group):\n",
    "    value = group['value']\n",
    "    if OPTIMIZATION_GOAL == 'minimize':\n",
    "        best_value = np.nanmin(value)\n",
    "        # Formula: (current - best) / best\n",
    "        group['relative_diff'] = (value - best_value) / best_value\n",
    "    else: # maximize\n",
    "        best_value = np.nanmax(value)\n",
    "        # Formula: (best - current) / best  (to measure drop from the best)\n",
    "        group['relative_diff'] = (best_value - value) / best_value\n",
    "    return group\n",
    "\n",
    "# Group by each individual experiment (task_id and split_method) and apply the function\n",
    "# This calculates the relative difference for each model compared to the BEST model\n",
    "# within that specific experiment.\n",
    "df_relative = df_metric.groupby(['task_id', 'split_method']).apply(calculate_relative_difference)\n",
    "\n",
    "\n",
    "# --- 3. Aggregate the Results and Create the Final Plot ---\n",
    "\n",
    "# Now, group by the model to see its average performance across ALL experiments\n",
    "# We multiply by 100 to express the difference as a percentage\n",
    "df_relative['relative_diff_pct'] = 100 * df_relative['relative_diff']\n",
    "summary_stats = df_relative.groupby('model')['relative_diff_pct'].agg(['mean', 'median', 'std']).reset_index()\n",
    "summary_stats.rename(columns={'mean': 'Mean', 'median': 'Median', 'std': 'Standard Deviation', 'model': 'Method'}, inplace=True)\n",
    "\n",
    "\n",
    "# Reorder the methods for a clean plot (same as the notebook)\n",
    "# You can customize this order\n",
    "method_order = [\n",
    "    'ConstantPredictor', 'LogisticRegressor', 'GAM', 'rf', 'boosted_trees', \n",
    "    'engression', 'MLP', 'ResNet', 'FTTrans'\n",
    "]\n",
    "# Note: I've used your model names like 'LogisticRegressor'. Adjust if needed.\n",
    "\n",
    "summary_df = summary_stats.copy()\n",
    "summary_df['Method'] = pd.Categorical(summary_df['Method'], categories=method_order, ordered=True)\n",
    "summary_df.sort_values('Method', inplace=True)\n",
    "\n",
    "# Replace model names with more readable labels for the plot\n",
    "summary_df['Method'] = summary_df['Method'].replace({\n",
    "    'rf': 'random forest', \n",
    "    'boosted_trees': 'boosted trees', \n",
    "    'LogisticRegressor': 'logistic regression', \n",
    "    'ConstantPredictor': 'constant',\n",
    "    'FTTrans': 'FT-Transformer'\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Summary DataFrame for Plotting:\")\n",
    "print(summary_df)\n",
    "\n",
    "# --- 4. Plot the final results ---\n",
    "plt.figure(figsize=(10, 8)) # Adjust figure size for better readability\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "sns.scatterplot(data=summary_df, x='Method', y='Mean', color='black', s=100) # s increases marker size\n",
    "\n",
    "plt.ylabel('Average relative difference to the best test score (in %)')\n",
    "plt.xlabel('Method')\n",
    "plt.title(f'Average Performance Across All Extrapolation Methods (Metric: {METRIC_TO_ANALYZE})')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout() # Adjust layout to make room for labels\n",
    "\n",
    "# Create a directory for pictures if it doesn't exist\n",
    "os.makedirs('PICTURES', exist_ok=True)\n",
    "plt.savefig(f'PICTURES/average_performance_{METRIC_TO_ANALYZE}.png')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
