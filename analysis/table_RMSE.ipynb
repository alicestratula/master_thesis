{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "632ff37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_results.csv\")\n",
    "df = df[df[\"split_method\"] != \"random_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc69909",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "avg_per_task = dfm.groupby(\"task_id\")[\"value\"].mean()\n",
    "\n",
    "avg_per_task = avg_per_task.reset_index(name=f\"avg_{metric}\")\n",
    "avg_per_task.to_csv(f\"avg_{metric}_per_task.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16ab74",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc758d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "pivot_rmse = dfm.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# 4) (Optional) write out to CSV\n",
    "pivot_rmse.to_csv(f\"avg_{metric}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eedafed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (RMSE):\n",
      "model\n",
      "ConstantPredictor          280.01\n",
      "Engression                  78.69\n",
      "FTTransformer               43.66\n",
      "LGBMRegressor               35.15\n",
      "LinearRegressor          92264.08\n",
      "MLP                        133.95\n",
      "RandomForestRegressor       39.83\n",
      "ResNet                     109.58\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_rmse = avg_relative_diff(pivot_rmse, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (RMSE):\")\n",
    "print(avg_diff_rmse.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96611d6",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"CRPS\"\n",
    "dfm1   = df[df[\"metric\"] == metric1]\n",
    "\n",
    "pivot_crps = dfm1.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# 4) (Optional) write out to CSV\n",
    "pivot_crps.to_csv(f\"avg_{metric1}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5492177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (CRPS):\n",
      "model\n",
      "ConstantPredictor          371.53\n",
      "DGBT                        42.65\n",
      "DRF                         62.20\n",
      "Engression               15127.91\n",
      "FTTransformer               46.80\n",
      "LGBMRegressor               42.55\n",
      "LinearRegressor          71983.54\n",
      "MLP                         54.37\n",
      "RandomForestRegressor       50.31\n",
      "ResNet                     123.53\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_crps = avg_relative_diff(pivot_crps, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (CRPS):\")\n",
    "print(avg_diff_crps.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454a3ce",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26bf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2 = \"LOGLOSS\"\n",
    "dfm2   = df[df[\"metric\"] == metric2]\n",
    "\n",
    "pivot_ll = dfm2.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# 4) (Optional) write out to CSV\n",
    "pivot_ll.to_csv(f\"avg_{metric2}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e31ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (LogLoss):\n",
      "model\n",
      "ConstantPredictor          371.53\n",
      "DGBT                        42.65\n",
      "DRF                         62.20\n",
      "Engression               15127.91\n",
      "FTTransformer               46.80\n",
      "LGBMRegressor               42.55\n",
      "LinearRegressor          71983.54\n",
      "MLP                         54.37\n",
      "RandomForestRegressor       50.31\n",
      "ResNet                     123.53\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_ll = avg_relative_diff(pivot_ll, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (LogLoss):\")\n",
    "print(avg_diff_ll.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b2ee",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4ddf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = \"Accuracy\"\n",
    "dfm3   = df[df[\"metric\"] == metric3]\n",
    "\n",
    "pivot_acc = dfm3.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_acc.to_csv(f\"avg_{metric3}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3643c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average relative difference (Accuracy):\n",
      "ConstantPredictor        NaN\n",
      "Engression               NaN\n",
      "FTTransformer            NaN\n",
      "LGBMClassifier           NaN\n",
      "LogisticRegressor        NaN\n",
      "MLP                      NaN\n",
      "RandomForestClassifier   NaN\n",
      "ResNet                   NaN\n",
      "361055                   NaN\n",
      "361060                   NaN\n",
      "361061                   NaN\n",
      "361062                   NaN\n",
      "361063                   NaN\n",
      "361065                   NaN\n",
      "361066                   NaN\n",
      "361068                   NaN\n",
      "361069                   NaN\n",
      "361070                   NaN\n",
      "361110                   NaN\n",
      "361111                   NaN\n",
      "361113                   NaN\n",
      "361273                   NaN\n",
      "361274                   NaN\n",
      "361275                   NaN\n",
      "361276                   NaN\n",
      "361277                   NaN\n",
      "361278                   NaN\n",
      "361282                   NaN\n",
      "361283                   NaN\n",
      "361285                   NaN\n",
      "361286                   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_accuracy = avg_relative_diff(pivot_acc, error_metric=False)\n",
    "\n",
    "print(\"\\nAverage relative difference (Accuracy):\")\n",
    "print(avg_diff_accuracy.round(2))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
