{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632ff37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_full.csv\")\n",
    "df = df[df[\"split_method\"] == \"random_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1a8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"results_full.csv\")\n",
    "latex_table = df.to_latex(index=False,escape = True, float_format=\"%.2e\")  # or adjust format\n",
    "with open(\"results_table_IP.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5807f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'ConstantPredictor': 'Const.',\n",
    "    'FTTransformer': 'FT-Trans.',\n",
    "    'LGBMRegressor': 'GBT',\n",
    "    'LGBMClassifier': 'GBT',\n",
    "    'LinearRegressor': 'Lin. Regr.',\n",
    "    'RandomForestRegressor': 'RF',\n",
    "    'RandomForestClassifier': 'RF',\n",
    "    'ResNet': 'ResNet',\n",
    "    'TabPFNRegressor': 'TabPFN',\n",
    "    'TabPFNClassifier': 'TabPFN',\n",
    "    'LogisticRegressor': 'Log. Regr.',\n",
    "    'GPBoost_LogLoss': 'GPBoost',\n",
    "    'GPBoost_Accuracy': 'GPBoost',\n",
    "    'GPBoost_CRPS': 'GPBoost',\n",
    "    'GPBoost_RMSE': 'GPBoost'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16ab74",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc758d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "pivot_rmse = dfm.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_rmse = pivot_rmse.rename(columns=rename_map)\n",
    "\n",
    "pivot_rmse.to_csv(f\"avg_{metric}_per_task_per_model_IP.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eedafed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (RMSE) - Interpolation :\n",
      "model\n",
      "Const.            358.24\n",
      "Engression        461.82\n",
      "FT-Trans.          33.54\n",
      "GPBoost       2350328.03\n",
      "GBT                43.05\n",
      "Lin. Regr.      76934.00\n",
      "MLP               116.86\n",
      "RF                 50.21\n",
      "ResNet            115.63\n",
      "TabPFN             19.09\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_rmse = avg_relative_diff(pivot_rmse, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (RMSE) - Interpolation :\")\n",
    "print(avg_diff_rmse.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b833d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from RMSE) - Interpolation:\n",
      "model\n",
      "Const.         8.62\n",
      "Engression    36.15\n",
      "FT-Trans.     63.39\n",
      "GPBoost       46.81\n",
      "GBT           74.74\n",
      "Lin. Regr.     7.79\n",
      "MLP           50.69\n",
      "RF            67.26\n",
      "ResNet        39.49\n",
      "TabPFN        90.46\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_rmse = normalized_accuracy(pivot_rmse, error_metric=True)\n",
    "print(\"Average normalized accuracy (from RMSE) - Interpolation:\")\n",
    "print((100 * avg_norm_acc_rmse).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be94171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (RMSE) - Interpolation:\n",
      "model\n",
      "Const.        8.92\n",
      "Engression    6.14\n",
      "FT-Trans.     4.30\n",
      "GPBoost       5.58\n",
      "GBT           3.64\n",
      "Lin. Regr.    8.28\n",
      "MLP           5.11\n",
      "RF            4.11\n",
      "ResNet        6.14\n",
      "TabPFN        1.94\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_rmse = avg_rank(pivot_rmse, error_metric=True)\n",
    "print(\"Average rank (RMSE) - Interpolation:\")\n",
    "print(avg_rank_rmse.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37f782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff    = avg_relative_diff(pivot_rmse, error_metric=True)           # in %\n",
    "avg_acc     = normalized_accuracy(pivot_rmse, error_metric=True) * 100   # convert to % \n",
    "avg_rank    = avg_rank(pivot_rmse, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff, avg_acc, avg_rank],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries = pd.concat([pivot_rmse, metrics])\n",
    "\n",
    "out = pivot_with_summaries.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out.to_csv(f\"avg_{metric}_with_summary_IP.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex = out.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{metric}_IP_results.tex\",\"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96611d6",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f54dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"CRPS\"\n",
    "dfm1   = df[df[\"metric\"] == metric1]\n",
    "\n",
    "pivot_crps = dfm1.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_crps = pivot_crps.rename(columns=rename_map)\n",
    "pivot_crps.to_csv(f\"avg_{metric1}_per_task_per_model_IP.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5492177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (CRPS) - Interpolation:\n",
      "model\n",
      "Const.            578.10\n",
      "DGBT               59.53\n",
      "DRF                77.18\n",
      "Engression        453.08\n",
      "FT-Trans.          49.86\n",
      "GPBoost       2467136.32\n",
      "GBT                58.79\n",
      "Lin. Regr.      54390.20\n",
      "MLP                84.79\n",
      "RF                 68.99\n",
      "ResNet            127.33\n",
      "TabPFN             22.61\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_crps = avg_relative_diff(pivot_crps, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (CRPS) - Interpolation:\")\n",
    "print(avg_diff_crps.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be168eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from CRPS) - Interpolation:\n",
      "model\n",
      "Const.         4.44\n",
      "DGBT          68.51\n",
      "DRF           70.95\n",
      "Engression    21.39\n",
      "FT-Trans.     59.16\n",
      "GPBoost       47.69\n",
      "GBT           68.47\n",
      "Lin. Regr.     7.30\n",
      "MLP           52.16\n",
      "RF            60.23\n",
      "ResNet        46.89\n",
      "TabPFN        81.94\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_crps = normalized_accuracy(pivot_crps, error_metric=True)\n",
    "print(\"Average normalized accuracy (from CRPS) - Interpolation:\")\n",
    "print((100 * avg_norm_acc_crps).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7392f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (CRPS) - Interpolation:\n",
      "model\n",
      "Const.        10.89\n",
      "DGBT           4.28\n",
      "DRF            4.86\n",
      "Engression     9.25\n",
      "FT-Trans.      5.24\n",
      "GPBoost        7.03\n",
      "GBT            4.33\n",
      "Lin. Regr.     9.92\n",
      "MLP            6.53\n",
      "RF             5.44\n",
      "ResNet         6.75\n",
      "TabPFN         2.50\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_crps = avg_rank(pivot_crps, error_metric=True)\n",
    "print(\"Average rank (CRPS) - Interpolation:\")\n",
    "print(avg_rank_crps.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c67f3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff1    = avg_relative_diff(pivot_crps, error_metric=True)           # in %\n",
    "avg_acc1     = normalized_accuracy(pivot_crps, error_metric=True) * 100   # convert to % \n",
    "avg_rank1    = avg_rank(pivot_crps, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff1, avg_acc1, avg_rank1],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries1 = pd.concat([pivot_crps, metrics])\n",
    "\n",
    "out1 = pivot_with_summaries1.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out1.to_csv(f\"avg_{metric1}_with_summary_IP.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex1 = out1.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{metric1}_IP_results.tex\",\"w\") as f:\n",
    "    f.write(latex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454a3ce",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26bf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2 = \"LogLoss\"\n",
    "dfm2   = df[df[\"metric\"] == metric2]\n",
    "\n",
    "pivot_ll = dfm2.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_ll = pivot_ll.rename(columns=rename_map)\n",
    "pivot_ll.to_csv(f\"avg_{metric2}_per_task_per_model_IP.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e31ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (LogLoss) - Interpolation:\n",
      "model\n",
      "Const.        265.71\n",
      "Engression    173.35\n",
      "FT-Trans.      15.68\n",
      "GPBoost        79.10\n",
      "GBT           231.22\n",
      "Log. Regr.    153.71\n",
      "MLP            20.67\n",
      "RF            231.22\n",
      "ResNet         31.34\n",
      "TabPFN          3.32\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_ll = avg_relative_diff(pivot_ll, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (LogLoss) - Interpolation:\")\n",
    "print(avg_diff_ll.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ddaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from LogLoss) - Interpolation:\n",
      "model\n",
      "Const.         6.35\n",
      "Engression    26.54\n",
      "FT-Trans.     71.88\n",
      "GPBoost       65.37\n",
      "GBT            6.63\n",
      "Log. Regr.    33.22\n",
      "MLP           61.39\n",
      "RF             6.63\n",
      "ResNet        58.57\n",
      "TabPFN        99.15\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_ll = normalized_accuracy(pivot_ll, error_metric=True)\n",
    "print(\"Average normalized accuracy (from LogLoss) - Interpolation:\")\n",
    "print((100 * avg_norm_acc_ll).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f247649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (LogLoss) - Interpolation:\n",
      "model\n",
      "Const.        9.00\n",
      "Engression    6.70\n",
      "FT-Trans.     3.55\n",
      "GPBoost       3.95\n",
      "GBT           7.41\n",
      "Log. Regr.    6.48\n",
      "MLP           4.09\n",
      "RF            7.41\n",
      "ResNet        4.52\n",
      "TabPFN        1.35\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_ll = avg_rank(pivot_ll, error_metric=True)\n",
    "print(\"Average rank (LogLoss) - Interpolation:\")\n",
    "print(avg_rank_ll.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "870b586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff2    = avg_relative_diff(pivot_ll, error_metric=True)           # in %\n",
    "avg_acc2     = normalized_accuracy(pivot_ll, error_metric=True) * 100   # convert to % \n",
    "avg_rank2    = avg_rank(pivot_ll, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff2, avg_acc2, avg_rank2],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries2 = pd.concat([pivot_ll, metrics])\n",
    "\n",
    "out2 = pivot_with_summaries2.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out2.to_csv(f\"avg_{metric2}_with_summary_IP.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex2 = out2.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{metric2}_IP_results.tex\",\"w\") as f:\n",
    "    f.write(latex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b2ee",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ddf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = \"Accuracy\"\n",
    "dfm3   = df[df[\"metric\"] == metric3]\n",
    "\n",
    "pivot_acc = dfm3.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_acc = pivot_acc.rename(columns=rename_map)\n",
    "pivot_acc.to_csv(f\"avg_{metric3}_per_task_per_model_IP.csv\", float_format=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3643c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average relative difference (Accuracy) - Interpolation:\n",
      "model\n",
      "Const.        43.91\n",
      "Engression    18.44\n",
      "FT-Trans.      3.29\n",
      "GPBoost        6.86\n",
      "GBT            1.08\n",
      "Log. Regr.     9.14\n",
      "MLP            4.11\n",
      "RF             2.12\n",
      "ResNet         3.89\n",
      "TabPFN         0.19\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)     \n",
    "                         .mul(-1)                         \n",
    "                         .div(best_per_task , axis=0)\n",
    "                         * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_accuracy = avg_relative_diff(pivot_acc, error_metric=False)\n",
    "\n",
    "print(\"\\nAverage relative difference (Accuracy) - Interpolation:\")\n",
    "print(avg_diff_accuracy.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a47b5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from Accuracy) - Interpolation:\n",
      "model\n",
      "Const.         0.000\n",
      "Engression     1.240\n",
      "FT-Trans.     62.371\n",
      "GPBoost       29.618\n",
      "GBT           88.401\n",
      "Log. Regr.    10.083\n",
      "MLP           50.767\n",
      "RF            78.002\n",
      "ResNet        55.907\n",
      "TabPFN        96.149\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_acc = normalized_accuracy(pivot_acc, error_metric=False)\n",
    "print(\"Average normalized accuracy (from Accuracy) - Interpolation:\")\n",
    "print((100 * avg_norm_acc_acc).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b70d20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (Accuracy) -Interpolation:\n",
      "model\n",
      "Const.        9.91\n",
      "Engression    8.74\n",
      "FT-Trans.     4.27\n",
      "GPBoost       6.41\n",
      "GBT           2.65\n",
      "Log. Regr.    7.43\n",
      "MLP           5.17\n",
      "RF            3.43\n",
      "ResNet        4.91\n",
      "TabPFN        1.65\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_acc = avg_rank(pivot_acc, error_metric=False)\n",
    "print(\"Average rank (Accuracy) -Interpolation:\")\n",
    "print(avg_rank_acc.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae76ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff3    = avg_relative_diff(pivot_acc, error_metric=False)           # in %\n",
    "avg_acc3     = normalized_accuracy(pivot_acc, error_metric=False) * 100   # convert to % \n",
    "avg_rank3    = avg_rank(pivot_acc, error_metric=False)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff3, avg_acc3, avg_rank3],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries3 = pd.concat([pivot_acc, metrics])\n",
    "\n",
    "out3 = pivot_with_summaries3.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out3.to_csv(f\"avg_{metric3}_with_summary_IP.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex3 = out3.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{metric3}_IP_results.tex\",\"w\") as f:\n",
    "    f.write(latex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e742b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
