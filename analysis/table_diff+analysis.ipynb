{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632ff37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_results.csv\")\n",
    "df = df[df[\"split_method\"] != \"random_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc69909",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "avg_per_task = dfm.groupby(\"task_id\")[\"value\"].mean()\n",
    "\n",
    "avg_per_task = avg_per_task.reset_index(name=f\"avg_{metric}\")\n",
    "avg_per_task.to_csv(f\"avg_{metric}_per_task.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16ab74",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc758d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "pivot_rmse = dfm.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_rmse.to_csv(f\"avg_{metric}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedafed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (RMSE):\n",
      "model\n",
      "ConstantPredictor           332.93\n",
      "Engression                 9070.44\n",
      "FTTransformer                49.61\n",
      "LGBMRegressor                47.43\n",
      "LinearRegressor          103915.06\n",
      "MLP                         149.23\n",
      "RandomForestRegressor        54.33\n",
      "ResNet                      124.98\n",
      "TabPFNRegressor              21.36\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_rmse = avg_relative_diff(pivot_rmse, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (RMSE):\")\n",
    "print(avg_diff_rmse.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b833d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from RMSE):\n",
      "model\n",
      "ConstantPredictor         9.07\n",
      "Engression               38.39\n",
      "FTTransformer            62.46\n",
      "LGBMRegressor            69.62\n",
      "LinearRegressor           4.35\n",
      "MLP                      46.72\n",
      "RandomForestRegressor    60.42\n",
      "ResNet                   37.16\n",
      "TabPFNRegressor          89.75\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_rmse = normalized_accuracy(pivot_rmse, error_metric=True)\n",
    "print(\"Average normalized accuracy (from RMSE):\")\n",
    "print((100 * avg_norm_acc_rmse).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be94171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (RMSE):\n",
      "model\n",
      "ConstantPredictor        7.69\n",
      "Engression               5.31\n",
      "FTTransformer            3.91\n",
      "LGBMRegressor            3.36\n",
      "LinearRegressor          7.42\n",
      "MLP                      5.11\n",
      "RandomForestRegressor    4.08\n",
      "ResNet                   5.37\n",
      "TabPFNRegressor          2.03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_rmse = avg_rank(pivot_rmse, error_metric=True)\n",
    "print(\"Average rank (RMSE):\")\n",
    "print(avg_rank_rmse.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96611d6",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"CRPS\"\n",
    "dfm1   = df[df[\"metric\"] == metric1]\n",
    "\n",
    "pivot_crps = dfm1.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# 4) (Optional) write out to CSV\n",
    "pivot_crps.to_csv(f\"avg_{metric1}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5492177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (CRPS):\n",
      "model\n",
      "ConstantPredictor          556.73\n",
      "DGBT                        64.48\n",
      "DRF                         84.20\n",
      "Engression               14765.58\n",
      "FTTransformer               62.12\n",
      "LGBMRegressor               65.11\n",
      "LinearRegressor          72921.62\n",
      "MLP                         83.01\n",
      "RandomForestRegressor       75.62\n",
      "ResNet                     156.40\n",
      "TabPFNRegressor             25.32\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_crps = avg_relative_diff(pivot_crps, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (CRPS):\")\n",
    "print(avg_diff_crps.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be168eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from CRPS):\n",
      "model\n",
      "ConstantPredictor         5.75\n",
      "DGBT                     59.88\n",
      "DRF                      56.96\n",
      "Engression               53.80\n",
      "FTTransformer            53.79\n",
      "LGBMRegressor            56.74\n",
      "LinearRegressor           6.68\n",
      "MLP                      34.01\n",
      "RandomForestRegressor    43.87\n",
      "ResNet                   29.95\n",
      "TabPFNRegressor          76.39\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_crps = normalized_accuracy(pivot_crps, error_metric=True)\n",
    "print(\"Average normalized accuracy (from CRPS):\")\n",
    "print((100 * avg_norm_acc_crps).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7392f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (CRPS):\n",
      "model\n",
      "ConstantPredictor        10.00\n",
      "DGBT                      4.14\n",
      "DRF                       4.86\n",
      "Engression                5.20\n",
      "FTTransformer             5.00\n",
      "LGBMRegressor             4.56\n",
      "LinearRegressor           9.19\n",
      "MLP                       6.89\n",
      "RandomForestRegressor     5.72\n",
      "ResNet                    6.83\n",
      "TabPFNRegressor           2.72\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_crps = avg_rank(pivot_crps, error_metric=True)\n",
    "print(\"Average rank (CRPS):\")\n",
    "print(avg_rank_crps.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454a3ce",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26bf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2 = \"LogLoss\"\n",
    "dfm2   = df[df[\"metric\"] == metric2]\n",
    "\n",
    "pivot_ll = dfm2.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_ll.to_csv(f\"avg_{metric2}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e31ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (LogLoss):\n",
      "model\n",
      "ConstantPredictor         13722.54\n",
      "Engression                  200.07\n",
      "FTTransformer                17.19\n",
      "LGBMClassifier              267.34\n",
      "LogisticRegressor           185.24\n",
      "MLP                          22.99\n",
      "RandomForestClassifier      267.34\n",
      "ResNet                       34.74\n",
      "TabPFNClassifier              4.92\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_ll = avg_relative_diff(pivot_ll, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (LogLoss):\")\n",
    "print(avg_diff_ll.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ddaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from LogLoss):\n",
      "model\n",
      "ConstantPredictor          0.00\n",
      "Engression                31.36\n",
      "FTTransformer             70.78\n",
      "LGBMClassifier            12.07\n",
      "LogisticRegressor         33.00\n",
      "MLP                       62.61\n",
      "RandomForestClassifier    12.07\n",
      "ResNet                    60.47\n",
      "TabPFNClassifier          99.31\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_ll = normalized_accuracy(pivot_ll, error_metric=True)\n",
    "print(\"Average normalized accuracy (from LogLoss):\")\n",
    "print((100 * avg_norm_acc_ll).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f247649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (LogLoss):\n",
      "model\n",
      "ConstantPredictor         8.96\n",
      "Engression                5.52\n",
      "FTTransformer             3.18\n",
      "LGBMClassifier            6.41\n",
      "LogisticRegressor         5.48\n",
      "MLP                       3.70\n",
      "RandomForestClassifier    6.41\n",
      "ResNet                    3.74\n",
      "TabPFNClassifier          1.35\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_ll = avg_rank(pivot_ll, error_metric=True)\n",
    "print(\"Average rank (LogLoss):\")\n",
    "print(avg_rank_ll.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b2ee",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ddf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = \"Accuracy\"\n",
    "dfm3   = df[df[\"metric\"] == metric3]\n",
    "\n",
    "pivot_acc = dfm3.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_acc.to_csv(f\"avg_{metric3}_per_task_per_model.csv\", float_format=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3643c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average relative difference (Accuracy):\n",
      "model\n",
      "ConstantPredictor         45.96\n",
      "Engression                17.99\n",
      "FTTransformer              3.30\n",
      "LGBMClassifier             1.17\n",
      "LogisticRegressor          9.17\n",
      "MLP                        4.32\n",
      "RandomForestClassifier     2.06\n",
      "ResNet                     4.11\n",
      "TabPFNClassifier           0.24\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)     \n",
    "                         .mul(-1)                         \n",
    "                         .div(best_per_task , axis=0)\n",
    "                         * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_accuracy = avg_relative_diff(pivot_acc, error_metric=False)\n",
    "\n",
    "print(\"\\nAverage relative difference (Accuracy):\")\n",
    "print(avg_diff_accuracy.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47b5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from Accuracy):\n",
      "model\n",
      "ConstantPredictor          0.000\n",
      "Engression                 2.183\n",
      "FTTransformer             57.816\n",
      "LGBMClassifier            85.567\n",
      "LogisticRegressor          9.175\n",
      "MLP                       45.448\n",
      "RandomForestClassifier    75.468\n",
      "ResNet                    50.169\n",
      "TabPFNClassifier          94.547\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_acc = normalized_accuracy(pivot_acc, error_metric=False)\n",
    "print(\"Average normalized accuracy (from Accuracy):\")\n",
    "print((100 * avg_norm_acc_acc).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70d20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (Accuracy):\n",
      "model\n",
      "ConstantPredictor         8.96\n",
      "Engression                7.70\n",
      "FTTransformer             4.23\n",
      "LGBMClassifier            2.61\n",
      "LogisticRegressor         6.65\n",
      "MLP                       4.87\n",
      "RandomForestClassifier    3.26\n",
      "ResNet                    4.74\n",
      "TabPFNClassifier          1.78\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_acc = avg_rank(pivot_acc, error_metric=False)\n",
    "print(\"Average rank (Accuracy):\")\n",
    "print(avg_rank_acc.round(2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
