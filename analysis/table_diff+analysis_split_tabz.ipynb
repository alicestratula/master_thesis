{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "632ff37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_full.csv\")\n",
    "df = df[df[\"split_method\"] == \"gower_split\"]\n",
    "df = df[df['suite_id'] == 334]\n",
    "split = \"gower_split\"\n",
    "type = 334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6872d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'ConstantPredictor': 'Const.',\n",
    "    'FTTransformer': 'FT-Trans.',\n",
    "    'LGBMRegressor': 'GBT',\n",
    "    'LGBMClassifier': 'GBT',\n",
    "    'LinearRegressor': 'Lin. Regr.',\n",
    "    'RandomForestRegressor': 'RF',\n",
    "    'RandomForestClassifier': 'RF',\n",
    "    'ResNet': 'ResNet',\n",
    "    'TabPFNRegressor': 'TabPFN',\n",
    "    'TabPFNClassifier': 'TabPFN',\n",
    "    'LogisticRegressor': 'Log. Regr.',\n",
    "    'GPBoost_LogLoss': 'GP',\n",
    "    'GPBoost_Accuracy': 'GP',\n",
    "    'GPBoost_CRPS': 'GP',\n",
    "    'GPBoost_RMSE': 'GP'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16ab74",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc758d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "pivot_rmse = dfm.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_rmse = pivot_rmse.rename(columns=rename_map)\n",
    "\n",
    "pivot_rmse.to_csv(f\"avg_{metric}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eedafed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (RMSE):\n",
      "model\n",
      "Const.          335.43\n",
      "Engression     2589.68\n",
      "FT-Trans.        35.96\n",
      "GPBoost         106.00\n",
      "GBT             141.53\n",
      "Lin. Regr.    96189.89\n",
      "MLP             367.64\n",
      "RF              153.44\n",
      "ResNet          120.62\n",
      "TabPFN           24.50\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_rmse = avg_relative_diff(pivot_rmse, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (RMSE):\")\n",
    "print(avg_diff_rmse.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c6b833d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from RMSE):\n",
      "model\n",
      "Const.        15.72\n",
      "Engression    42.19\n",
      "FT-Trans.     61.80\n",
      "GPBoost       28.86\n",
      "GBT           75.12\n",
      "Lin. Regr.     0.00\n",
      "MLP           45.42\n",
      "RF            60.94\n",
      "ResNet        44.72\n",
      "TabPFN        95.74\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_rmse = normalized_accuracy(pivot_rmse, error_metric=True)\n",
    "print(\"Average normalized accuracy (from RMSE):\")\n",
    "print((100 * avg_norm_acc_rmse).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9be94171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (RMSE):\n",
      "model\n",
      "Const.        8.29\n",
      "Engression    5.59\n",
      "FT-Trans.     3.93\n",
      "GPBoost       6.35\n",
      "GBT           3.24\n",
      "Lin. Regr.    9.18\n",
      "MLP           5.65\n",
      "RF            4.35\n",
      "ResNet        5.71\n",
      "TabPFN        1.65\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_rmse = avg_rank(pivot_rmse, error_metric=True)\n",
    "print(\"Average rank (RMSE):\")\n",
    "print(avg_rank_rmse.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c37f782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff    = avg_relative_diff(pivot_rmse, error_metric=True)           # in %\n",
    "avg_acc     = normalized_accuracy(pivot_rmse, error_metric=True) * 100   # convert to % \n",
    "avg_rank    = avg_rank(pivot_rmse, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff, avg_acc, avg_rank],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries = pd.concat([pivot_rmse, metrics])\n",
    "\n",
    "out = pivot_with_summaries.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out.to_csv(f\"avg_{metric}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex = out.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{type}_{split}_{metric}_results.tex\",\"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96611d6",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f54dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"CRPS\"\n",
    "dfm1   = df[df[\"metric\"] == metric1]\n",
    "\n",
    "pivot_crps = dfm1.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_crps = pivot_crps.rename(columns=rename_map)\n",
    "pivot_crps.to_csv(f\"avg_{metric1}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5492177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (CRPS):\n",
      "model\n",
      "Const.          743.72\n",
      "DGBT             71.88\n",
      "DRF             152.01\n",
      "Engression     2359.59\n",
      "FT-Trans.        26.64\n",
      "GPBoost         310.01\n",
      "GBT              94.24\n",
      "Lin. Regr.    49160.59\n",
      "MLP             290.49\n",
      "RF              108.34\n",
      "ResNet          367.46\n",
      "TabPFN           15.10\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_crps = avg_relative_diff(pivot_crps, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (CRPS):\")\n",
    "print(avg_diff_crps.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "be168eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from CRPS):\n",
      "model\n",
      "Const.        14.70\n",
      "DGBT          71.29\n",
      "DRF           70.29\n",
      "Engression    18.80\n",
      "FT-Trans.     77.77\n",
      "GPBoost       40.86\n",
      "GBT           71.46\n",
      "Lin. Regr.     6.42\n",
      "MLP           57.00\n",
      "RF            63.31\n",
      "ResNet        54.12\n",
      "TabPFN        89.68\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_crps = normalized_accuracy(pivot_crps, error_metric=True)\n",
    "print(\"Average normalized accuracy (from CRPS):\")\n",
    "print((100 * avg_norm_acc_crps).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7392f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (CRPS):\n",
      "model\n",
      "Const.        10.41\n",
      "DGBT           4.41\n",
      "DRF            4.71\n",
      "Engression     9.47\n",
      "FT-Trans.      3.71\n",
      "GPBoost        7.82\n",
      "GBT            4.65\n",
      "Lin. Regr.    10.24\n",
      "MLP            6.47\n",
      "RF             5.59\n",
      "ResNet         6.88\n",
      "TabPFN         2.18\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_crps = avg_rank(pivot_crps, error_metric=True)\n",
    "print(\"Average rank (CRPS):\")\n",
    "print(avg_rank_crps.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c67f3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff1    = avg_relative_diff(pivot_crps, error_metric=True)           # in %\n",
    "avg_acc1     = normalized_accuracy(pivot_crps, error_metric=True) * 100   # convert to % \n",
    "avg_rank1    = avg_rank(pivot_crps, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff1, avg_acc1, avg_rank1],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries1 = pd.concat([pivot_crps, metrics])\n",
    "\n",
    "out1 = pivot_with_summaries1.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out1.to_csv(f\"avg_{metric1}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex1 = out1.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{type}_{split}_{metric1}_results.tex\",\"w\") as f:\n",
    "    f.write(latex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454a3ce",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26bf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2 = \"LogLoss\"\n",
    "dfm2   = df[df[\"metric\"] == metric2]\n",
    "\n",
    "pivot_ll = dfm2.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_ll = pivot_ll.rename(columns=rename_map)\n",
    "pivot_ll.to_csv(f\"avg_{metric2}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e31ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (LogLoss):\n",
      "model\n",
      "Const.         46.43\n",
      "Engression    152.45\n",
      "FT-Trans.       6.57\n",
      "GP             15.02\n",
      "GBT             5.32\n",
      "Log. Regr.     21.10\n",
      "MLP            18.39\n",
      "RF              5.50\n",
      "ResNet         14.12\n",
      "TabPFN          1.54\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_ll = avg_relative_diff(pivot_ll, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (LogLoss):\")\n",
    "print(avg_diff_ll.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8ddaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from LogLoss):\n",
      "model\n",
      "Const.         2.03\n",
      "Engression     0.00\n",
      "FT-Trans.     66.13\n",
      "GP            38.31\n",
      "GBT           80.11\n",
      "Log. Regr.    14.73\n",
      "MLP           40.90\n",
      "RF            77.53\n",
      "ResNet        59.10\n",
      "TabPFN        93.62\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_ll = normalized_accuracy(pivot_ll, error_metric=True)\n",
    "print(\"Average normalized accuracy (from LogLoss):\")\n",
    "print((100 * avg_norm_acc_ll).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f247649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (LogLoss):\n",
      "model\n",
      "Const.        9.14\n",
      "Engression    8.86\n",
      "FT-Trans.     4.17\n",
      "GP            6.14\n",
      "GBT           2.71\n",
      "Log. Regr.    7.43\n",
      "MLP           5.71\n",
      "RF            3.14\n",
      "ResNet        5.00\n",
      "TabPFN        1.86\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_ll = avg_rank(pivot_ll, error_metric=True)\n",
    "print(\"Average rank (LogLoss):\")\n",
    "print(avg_rank_ll.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "870b586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff2    = avg_relative_diff(pivot_ll, error_metric=True)           # in %\n",
    "avg_acc2     = normalized_accuracy(pivot_ll, error_metric=True) * 100   # convert to % \n",
    "avg_rank2    = avg_rank(pivot_ll, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff2, avg_acc2, avg_rank2],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries2 = pd.concat([pivot_ll, metrics])\n",
    "\n",
    "out2 = pivot_with_summaries2.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out2.to_csv(f\"avg_{metric2}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex2 = out2.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{type}_{split}_{metric2}_results.tex\",\"w\") as f:\n",
    "    f.write(latex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b2ee",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4ddf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = \"Accuracy\"\n",
    "dfm3   = df[df[\"metric\"] == metric3]\n",
    "\n",
    "pivot_acc = dfm3.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_acc = pivot_acc.rename(columns=rename_map)\n",
    "pivot_acc.to_csv(f\"avg_{metric3}_per_task_per_model.csv\", float_format=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3643c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average relative difference (Accuracy):\n",
      "model\n",
      "Const.        41.83\n",
      "Engression     8.03\n",
      "FT-Trans.      6.59\n",
      "GP             6.72\n",
      "GBT            2.30\n",
      "Log. Regr.     6.69\n",
      "MLP            3.49\n",
      "RF             2.50\n",
      "ResNet         3.54\n",
      "TabPFN         1.15\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)     \n",
    "                         .mul(-1)                         \n",
    "                         .div(best_per_task , axis=0)\n",
    "                         * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_accuracy = avg_relative_diff(pivot_acc, error_metric=False)\n",
    "\n",
    "print(\"\\nAverage relative difference (Accuracy):\")\n",
    "print(avg_diff_accuracy.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a47b5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from Accuracy):\n",
      "model\n",
      "Const.         0.000\n",
      "Engression    18.091\n",
      "FT-Trans.     29.494\n",
      "GP            15.650\n",
      "GBT           65.321\n",
      "Log. Regr.    13.713\n",
      "MLP           51.136\n",
      "RF            69.918\n",
      "ResNet        59.839\n",
      "TabPFN        82.953\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_acc = normalized_accuracy(pivot_acc, error_metric=False)\n",
    "print(\"Average normalized accuracy (from Accuracy):\")\n",
    "print((100 * avg_norm_acc_acc).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b70d20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (Accuracy):\n",
      "model\n",
      "Const.        9.86\n",
      "Engression    7.07\n",
      "FT-Trans.     6.08\n",
      "GP            7.29\n",
      "GBT           3.71\n",
      "Log. Regr.    6.86\n",
      "MLP           4.14\n",
      "RF            3.07\n",
      "ResNet        3.71\n",
      "TabPFN        2.64\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_acc = avg_rank(pivot_acc, error_metric=False)\n",
    "print(\"Average rank (Accuracy):\")\n",
    "print(avg_rank_acc.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae76ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff3    = avg_relative_diff(pivot_acc, error_metric=False)           # in %\n",
    "avg_acc3     = normalized_accuracy(pivot_acc, error_metric=False) * 100   # convert to % \n",
    "avg_rank3    = avg_rank(pivot_acc, error_metric=False)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff3, avg_acc3, avg_rank3],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries3 = pd.concat([pivot_acc, metrics])\n",
    "\n",
    "out3 = pivot_with_summaries3.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out3.to_csv(f\"avg_{metric3}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex3 = out3.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"{type}_{split}_{metric3}_results.tex\",\"w\") as f:\n",
    "    f.write(latex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652c70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
