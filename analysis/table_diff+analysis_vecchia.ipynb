{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632ff37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"vecchia_results.csv\")\n",
    "df = df[df[\"split_method\"] != \"random_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5807f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'ConstantPredictor': 'Const.',\n",
    "    'FTTransformer': 'FT-Trans.',\n",
    "    'LGBMRegressor': 'GBT',\n",
    "    'LGBMClassifier': 'GBT',\n",
    "    'LinearRegressor': 'Lin. Regr.',\n",
    "    'RandomForestRegressor': 'RF',\n",
    "    'RandomForestClassifier': 'RF',\n",
    "    'ResNet': 'ResNet',\n",
    "    'TabPFNRegressor': 'TabPFN',\n",
    "    'TabPFNClassifier': 'TabPFN',\n",
    "    'LogisticRegressor': 'Log. Regr.',\n",
    "    'GPBoost_LogLoss': 'GP',\n",
    "    'GPBoost_Accuracy': 'GP',\n",
    "    'GPBoost_CRPS': 'GP',\n",
    "    'GPBoost_RMSE': 'GP'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16ab74",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc758d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "dfm   = df[df[\"metric\"] == metric]\n",
    "\n",
    "pivot_rmse = dfm.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_rmse = pivot_rmse.rename(columns=rename_map)\n",
    "\n",
    "pivot_rmse.to_csv(f\"vecchia_avg_{metric}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedafed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (RMSE):\n",
      "model\n",
      "GP    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_rmse = avg_relative_diff(pivot_rmse, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (RMSE):\")\n",
    "print(avg_diff_rmse.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b833d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from RMSE):\n",
      "model\n",
      "GP   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_rmse = normalized_accuracy(pivot_rmse, error_metric=True)\n",
    "print(\"Average normalized accuracy (from RMSE):\")\n",
    "print((100 * avg_norm_acc_rmse).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be94171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (RMSE):\n",
      "model\n",
      "GP    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_rmse = avg_rank(pivot_rmse, error_metric=True)\n",
    "print(\"Average rank (RMSE):\")\n",
    "print(avg_rank_rmse.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37f782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff    = avg_relative_diff(pivot_rmse, error_metric=True)           # in %\n",
    "avg_acc     = normalized_accuracy(pivot_rmse, error_metric=True) * 100   # convert to % \n",
    "avg_rank    = avg_rank(pivot_rmse, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff, avg_acc, avg_rank],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries = pd.concat([pivot_rmse, metrics])\n",
    "\n",
    "out = pivot_with_summaries.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out.to_csv(f\"vecchia_avg_{metric}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex = out.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"vecchia_{metric}_results.tex\",\"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96611d6",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"CRPS\"\n",
    "dfm1   = df[df[\"metric\"] == metric1]\n",
    "\n",
    "pivot_crps = dfm1.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_crps = pivot_crps.rename(columns=rename_map)\n",
    "pivot_crps.to_csv(f\"vecchia_avg_{metric1}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5492177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (CRPS):\n",
      "model\n",
      "GP    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    # average across tasks (skipping any NaNs)\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_crps = avg_relative_diff(pivot_crps, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (CRPS):\")\n",
    "print(avg_diff_crps.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be168eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from CRPS):\n",
      "model\n",
      "GP   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_crps = normalized_accuracy(pivot_crps, error_metric=True)\n",
    "print(\"Average normalized accuracy (from CRPS):\")\n",
    "print((100 * avg_norm_acc_crps).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7392f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (CRPS):\n",
      "model\n",
      "GP    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_crps = avg_rank(pivot_crps, error_metric=True)\n",
    "print(\"Average rank (CRPS):\")\n",
    "print(avg_rank_crps.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67f3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff1    = avg_relative_diff(pivot_crps, error_metric=True)           # in %\n",
    "avg_acc1     = normalized_accuracy(pivot_crps, error_metric=True) * 100   # convert to % \n",
    "avg_rank1    = avg_rank(pivot_crps, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff1, avg_acc1, avg_rank1],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries1 = pd.concat([pivot_crps, metrics])\n",
    "\n",
    "out1 = pivot_with_summaries1.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out1.to_csv(f\"vecchia_avg_{metric1}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex1 = out1.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"vecchia_{metric1}_results.tex\",\"w\") as f:\n",
    "    f.write(latex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454a3ce",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference LOGLOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26bf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2 = \"LogLoss\"\n",
    "dfm2   = df[df[\"metric\"] == metric2]\n",
    "\n",
    "pivot_ll = dfm2.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot_ll = pivot_ll.rename(columns=rename_map)\n",
    "pivot_ll.to_csv(f\"vecchia_avg_{metric2}_per_task_per_model.csv\", float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e31ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative difference (LogLoss):\n",
      "model\n",
      "GP    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (best_per_task.sub(pivot, axis=0)\n",
    "                              .div(best_per_task, axis=0)\n",
    "                              * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_ll = avg_relative_diff(pivot_ll, error_metric=True)\n",
    "\n",
    "print(\"Average relative difference (LogLoss):\")\n",
    "print(avg_diff_ll.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ddaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from LogLoss):\n",
      "model\n",
      "GP   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .mul(-1)\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_ll = normalized_accuracy(pivot_ll, error_metric=True)\n",
    "print(\"Average normalized accuracy (from LogLoss):\")\n",
    "print((100 * avg_norm_acc_ll).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f247649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (LogLoss):\n",
      "model\n",
      "GP    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_ll = avg_rank(pivot_ll, error_metric=True)\n",
    "print(\"Average rank (LogLoss):\")\n",
    "print(avg_rank_ll.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "870b586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff2    = avg_relative_diff(pivot_ll, error_metric=True)           # in %\n",
    "avg_acc2     = normalized_accuracy(pivot_ll, error_metric=True) * 100   # convert to % \n",
    "avg_rank2    = avg_rank(pivot_ll, error_metric=True)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff2, avg_acc2, avg_rank2],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries2 = pd.concat([pivot_ll, metrics])\n",
    "\n",
    "out2 = pivot_with_summaries2.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out2.to_csv(f\"vecchia_avg_{metric2}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex2 = out2.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"vecchia_{metric2}_results.tex\",\"w\") as f:\n",
    "    f.write(latex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2b2ee",
   "metadata": {},
   "source": [
    "Average Difference and Average Relative Difference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4ddf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = \"Accuracy\"\n",
    "dfm3   = df[df[\"metric\"] == metric3]\n",
    "\n",
    "pivot_acc = dfm3.pivot_table(\n",
    "    index=\"task_id\",\n",
    "    columns=\"model\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "pivot_acc = pivot_acc.rename(columns=rename_map)\n",
    "pivot_acc.to_csv(f\"vecchia_avg_{metric3}_per_task_per_model.csv\", float_format=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3643c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average relative difference (Accuracy):\n",
      "model\n",
      "GP    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_relative_diff(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best_per_task = pivot.min(axis=1)   # best is minimum\n",
    "        # (v - best)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)\n",
    "                         .div(best_per_task, axis=0)\n",
    "                         * 100)\n",
    "    else:\n",
    "        best_per_task = pivot.max(axis=1)   # best is maximum\n",
    "        # (best - v)/best * 100\n",
    "        rel_diff = (pivot.sub(best_per_task, axis=0)     \n",
    "                         .mul(-1)                         \n",
    "                         .div(best_per_task , axis=0)\n",
    "                         * 100)\n",
    "\n",
    "    return rel_diff.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_diff_accuracy = avg_relative_diff(pivot_acc, error_metric=False)\n",
    "\n",
    "print(\"\\nAverage relative difference (Accuracy):\")\n",
    "print(avg_diff_accuracy.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a47b5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized accuracy (from Accuracy):\n",
      "model\n",
      "GP   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalized_accuracy(pivot: pd.DataFrame,\n",
    "                        error_metric: bool = True) -> pd.Series:\n",
    "\n",
    "    if error_metric:\n",
    "        best = pivot.min(axis=1)  \n",
    "\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nlargest(3).min(),\n",
    "            axis=1\n",
    "        )\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,       axis=0)    # mid[t] - pivot.loc[t, m]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        best = pivot.max(axis=1)\n",
    "        mid  = pivot.apply(\n",
    "            lambda s: s.dropna().nsmallest(3).max(),\n",
    "            axis=1\n",
    "        )\n",
    "        # 3) norm_acc per cell = (acc - mid) / (best - mid), clipped\n",
    "        norm = (\n",
    "            pivot\n",
    "            .rsub(mid,          axis=0)   # pivot.loc[t,m] - mid[t]\n",
    "            .div(mid - best, axis=0)\n",
    "            .clip(0, 1)\n",
    "        )\n",
    "\n",
    "    return norm.mean(axis=0)\n",
    "\n",
    "\n",
    "avg_norm_acc_acc = normalized_accuracy(pivot_acc, error_metric=False)\n",
    "print(\"Average normalized accuracy (from Accuracy):\")\n",
    "print((100 * avg_norm_acc_acc).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b70d20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank (Accuracy):\n",
      "model\n",
      "GP    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def avg_rank(pivot: pd.DataFrame, error_metric: bool = True) -> pd.Series:\n",
    "  \n",
    "    ranks = pivot.rank(\n",
    "        axis=1,\n",
    "        method=\"average\",   \n",
    "        ascending=error_metric\n",
    "    )\n",
    "\n",
    "    return ranks.mean(axis=0)\n",
    "\n",
    "avg_rank_acc = avg_rank(pivot_acc, error_metric=False)\n",
    "print(\"Average rank (Accuracy):\")\n",
    "print(avg_rank_acc.round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae76ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff3    = avg_relative_diff(pivot_acc, error_metric=False)           # in %\n",
    "avg_acc3     = normalized_accuracy(pivot_acc, error_metric=False) * 100   # convert to % \n",
    "avg_rank3    = avg_rank(pivot_acc, error_metric=False)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    [avg_diff3, avg_acc3, avg_rank3],\n",
    "    index=[\"Avg. diff.\", \"Avg. acc.\", \"Avg. rank.\"]\n",
    ")\n",
    "\n",
    "pivot_with_summaries3 = pd.concat([pivot_acc, metrics])\n",
    "\n",
    "out3 = pivot_with_summaries3.reset_index().rename(columns={\"index\":\"task_id\"})\n",
    "\n",
    "out3.to_csv(f\"vecchia_avg_{metric3}_with_summary.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "latex3 = out3.to_latex(index=False, escape=True, float_format=\"%.3f\")\n",
    "with open(f\"vecchia_{metric3}_results.tex\",\"w\") as f:\n",
    "    f.write(latex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652c70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
